---
layout: page
title:  Optimal Transport and Machine Learning  
description: NeurIPS 2021 Workshop. 13th of December 2021
background: '/img/color_transfer.jpg'
---

## Schedule

The workshop will alternate between invited speakers surveying the state of the art and contributed
talks presenting recent advances. The presentations will highlight the interplay between recent
theoretical advances, innovative efficient numerical solvers and successful applications in ML.

<style type="text/css">
.tg  {border:none;border-collapse:collapse;border-color:#ccc;border-spacing:0;}
.tg td{background-color:#fff;border-color:#ccc;border-style:solid;border-width:0px;color:#333;
  sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg th{background-color:#f0f0f0;border-color:#ccc;border-style:solid;border-width:0px;color:#333;
  sans-serif;font-size:14px;font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top}
.tg .tg-btxf{background-color:#f9f9f9;border-color:inherit;text-align:left;vertical-align:top}
</style>

<table class="tg" style="white-space:nowrap;">
<thead>
  <tr>
    <th class="tg-0pky"><b>Time</b> (CET) <br> GMT+1</th>
    <th class="tg-0pky"><b>Time</b> (EST) <br> GMT-5</th>
    <th class="tg-0pky"><b>What?</b></th>
    <th class="tg-0pky"><b>Who?</b></th>
    <th class="tg-0pky"></th>
  </tr>
</thead>
<tbody>
  <tr>
    <td class="tg-btxf"></td>
    <td class="tg-btxf"></td>
    <td class="tg-btxf"></td>
    <td class="tg-btxf"></td>
    <td class="tg-btxf"></td>
  </tr>
  <tr>
    <td class="tg-0pky">14:00 : 14:45</td>
    <td class="tg-0pky">08:00 : 08:45</td>
    <td class="tg-0pky">Plenary Speaker</td>
    <td class="tg-0pky">Lenaic Chizat</td>
    <td class="tg-0pky"><details>
    <summary>TBD</summary>
    <br>
    ...
    </details></td>
  </tr>
  <tr>
    <td class="tg-btxf">14:45 : 15:00</td>
    <td class="tg-btxf">08:45 : 09:00</td>
    <td class="tg-btxf">Oral</td>
    <td class="tg-btxf">Danilo Jimenez Rezende</td>
    <td class="tg-btxf"><details>
    <summary>Implicit Riemannian Concave Potential Maps</summary>
    <br>
    We are interested in the challenging problem of modelling densities on Riemannian manifolds with a known symmetry group using normalising flows. This has many potential applications in physical sciences such as molecular dynamics and quantum simulations. In this work we combine ideas from implicit neural layers and optimal transport theory to propose a generalisation of existing work on exponential map flows, Implicit Riemannian Concave Potential Maps, IRCPMs. IRCPMs have some nice properties such as simplicity of incorporating knowledge about symmetries and are less expensive then ODE-flows. We provide an initial theoretical analysis of its properties and layout sufficient conditions for stable optimisation. Finally, we illustrate the properties of IRCPMs with density learning experiments on tori and spheres.
    </details></td>
  </tr>
  <tr>
    <td class="tg-0pky">15:00 : 16:10</td>
    <td class="tg-0pky">09:00 : 10:10</td>
    <td class="tg-0pky">Plenary Speaker</td>
    <td class="tg-0pky">Alessio Figalli</td>
    <td class="tg-0pky"><details>
    <summary>Regularity Theory of Optimal Transport Maps</summary>
    <br>
    In optimal transport, understanding the regularity of optimal maps is an important topic. This lecture aims to present the regularity theory for optimal maps, explain the connection to Monge-Amp√®re type equations, and overview the most fundamental results available.
    </details></td>
  </tr>
  <tr>
    <td class="tg-btxf">16:10 : 16:35</td>
    <td class="tg-btxf">10:10 : 10:35</td>
    <td class="tg-btxf">Keynote Speaker</td>
    <td class="tg-btxf">Beatrice Acciaio</td>
    <td class="tg-btxf"><details>
    <summary>Generative Adversarial Learning with Adapted Distances</summary>
    <br>
    Generative Adversarial Networks (GANs) have proven to be a powerful framework for learning to draw samples from complex distributions. In this talk I will discuss the challenge of learning sequential data via GANs. This notably requires the choice of a loss function that reflects the discrepancy between (measures on) paths. To take on this task, we employ adapted versions of optimal transport distances, that result from imposing a temporal causality constraint on classical transport problems. This constraint provides a natural framework to parameterize the cost function that is learned by the discriminator as a robust (worst-case) distance. We then employ a modification of the empirical measure, to ensure consistency of the estimators. Following Genevay et al. (2018), we also include an entropic penalization term which allows for the use of the Sinkhorn algorithm when computing the optimal transport cost.
    </details></td>
  </tr>
  <tr>
    <td class="tg-0pky">16:35 : 17:15</td>
    <td class="tg-0pky">10:35 : 11:15</td>
    <td class="tg-0pky">Spotlights</td>
    <td class="tg-0pky"></td>
    <td class="tg-0pky"></td>
  </tr>
  <tr>
    <td class="tg-btxf">17:00 : 17:45</td>
    <td class="tg-btxf">11:00 : 11:45</td>
    <td class="tg-btxf">Poster Session</td>
    <td class="tg-btxf"></td>
    <td class="tg-btxf"></td>
  </tr>
  <tr>
    <td class="tg-0pky">17:45 : 18:30</td>
    <td class="tg-0pky">11:45 : 12:30</td>
    <td class="tg-0pky">Plenary Speaker</td>
    <td class="tg-0pky">Caroline Uhler</td>
    <td class="tg-0pky"><details>
    <summary>TBD</summary>
    <br>
    ...
    </details></td>
  </tr>
  <tr>
    <td class="tg-btxf">18:30 : 18:55</td>
    <td class="tg-btxf">12:30 : 12:55</td>
    <td class="tg-btxf">Keynote Speaker</td>
    <td class="tg-btxf">Chin-Wei Huang</td>
    <td class="tg-btxf"><details>
    <summary>Optimal Transport and Probability Flows</summary>
    <br>
    In this talk, I will present some recent work at the intersection of optimal transport (OT) and probability flows. Optimal transport is an elegant theory that has diverse downstream applications. For likelihood estimation in particular, there has been a recent interest in using parametric invertible models (aka normalizing flows) to approximate the data distribution of interest. I will present my recent work on parameterizing flows using a neural convex potential, which is inspired by Brenier's theorem. In addition, I will cover a few other recently proposed probability flow models related to OT.
    </details></td>
  </tr>
  <tr>
    <td class="tg-0pky">18:55 : 19:20</td>
    <td class="tg-0pky">12:55 : 13:20</td>
    <td class="tg-0pky">Keynote Speaker</td>
    <td class="tg-0pky">Yongxin Chen</td>
    <td class="tg-0pky"><details>
    <summary>Graphical Optimal Transport and its Applications</summary>
    <br>
    Multi-marginal optimal transport (MOT) is a generalization of optimal transport theory to settings with possibly more than two marginals. The computation of the solutions to MOT problems has been a longstanding challenge. In this talk, we introduce graphical optimal transport, a special class of MOT problems. We consider MOT problems from a probabilistic graphical model perspective and point out an elegant connection between the two when the underlying cost for optimal transport allows a graph structure. In particular, an entropy regularized MOT is equivalent to a Bayesian marginal inference problem for probabilistic graphical models with the additional requirement that some of the marginal distributions are specified. This relation on the one hand extends the optimal transport as well as the probabilistic graphical model theories, and on the other hand leads to fast algorithms for MOT by leveraging the well-developed algorithms in Bayesian inference. We will cover recent developments of graphical optimal transport in theory and algorithms. We will also go over several applications in aggregate filtering and mean field games.
    </details></td>
  </tr>
  <tr>
    <td class="tg-btxf">19:20 : 20:00</td>
    <td class="tg-btxf">13:20 : 14:00</td>
    <td class="tg-btxf">Poster Session</td>
    <td class="tg-btxf"></td>
    <td class="tg-btxf"></td>
  </tr>
  <tr>
    <td class="tg-0pky">20:00 : 20:25</td>
    <td class="tg-0pky">14:00 : 14:25</td>
    <td class="tg-0pky">Keynote Speaker</td>
    <td class="tg-0pky">Pinar Demetci</td>
    <td class="tg-0pky"><details>
    <summary>TBD</summary>
    <br>
    ...
    </details></td>
  </tr>
  <tr>
    <td class="tg-btxf">20:25 : 20:40</td>
    <td class="tg-btxf">14:25 : 14:40</td>
    <td class="tg-btxf">Oral</td>
    <td class="tg-btxf">Aram-Alexandre Pooladian</td>
    <td class="tg-btxf"><details>
    <summary>Entropic Estimation of Optimal Transport Maps</summary>
    <br>
    We develop a computationally tractable method for estimating the optimal map between two distributions over  with rigorous finite-sample guarantees. Leveraging an entropic version of Brenier's theorem, we show that our estimator---the barycentric projection of the optimal entropic plan---is easy to compute using Sinkhorn's algorithm. As a result, unlike current approaches for map estimation, which are slow to evaluate when the dimension or number of samples is large, our approach is parallelizable and extremely efficient even for massive data sets. Under smoothness assumptions on the optimal map, we show that our estimator enjoys comparable statistical performance to other estimators in the literature, but with much lower computational cost. We showcase the efficacy of our proposed estimator through numerical examples. Our proofs are based on a modified duality principle for entropic optimal transport and on a method for approximating optimal entropic plans due to Pal (2019).
    </details></td>
  </tr>
  <tr>
    <td class="tg-0pky">20:40 : 20:55</td>
    <td class="tg-0pky">14:40 : 14:55</td>
    <td class="tg-0pky">Oral</td>
    <td class="tg-0pky">Zaid Harchaoui, Lang Liu</td>
    <td class="tg-0pky"><details>
    <summary>Discrete Schr√∂dinger Bridges with Applications to Two-Sample Homogeneity Testing</summary>
    <br>
    We introduce an entropy-regularized statistic that defines a divergence between probability distributions. The statistic is the transport cost of a coupling which admits an expression as a weighted average of Monge couplings with respect to a Gibbs measure. This coupling is related to the static Schr√∂dinger bridge given a finite number of particles. We establish the asymptotic consistency of the statistic as the sample size goes to infinity and show that the population limit is the solution of F√∂llmer's entropy-regularized optimal transport. The proof technique relies on a chaos decomposition for paired samples. We illustrate the interest of the approach on the two-sample homogeneity testing problem.
    </details></td>
  </tr>
  <tr>
    <td class="tg-btxf">20:40 : 21:20</td>
    <td class="tg-btxf">14:40 : 15:20</td>
    <td class="tg-btxf">Keynote Speaker</td>
    <td class="tg-btxf">Yunan Yang</td>
    <td class="tg-btxf"><details>
    <summary>Benefits of using Optimal Transport in Computational Learning and Inversion</summary>
    <br>
    Understanding the generalization capacity has been a central topic in mathematical machine learning. In this talk, I will present a generalized weighted least-squares optimization method for computational learning and inversion with noisy data. In particular, using the Wasserstein metric as the objective function and implementing the Wasserstein gradient flow (or Wasserstein natural gradient descent method) fall into the framework. The weighting scheme encodes both a priori knowledge on the object to be learned and a strategy to weight the contribution of different data points in the loss function. We will see that appropriate weighting from prior knowledge can greatly improve the generalization capability of the learned model.
    </details></td>
  </tr>
  <tr>
    <td class="tg-0pky">21:20 : 21:25</td>
    <td class="tg-0pky">15:20 : 15:25</td>
    <td class="tg-0pky">Closing Remarks</td>
    <td class="tg-0pky"></td>
    <td class="tg-0pky"></td>
  </tr>
  <tr>
    <td class="tg-btxf">21:25 : 22:00</td>
    <td class="tg-btxf">15:25 : 16:00</td>
    <td class="tg-btxf">Poster Session</td>
    <td class="tg-btxf"></td>
    <td class="tg-btxf"></td>
  </tr>
</tbody>
</table>
